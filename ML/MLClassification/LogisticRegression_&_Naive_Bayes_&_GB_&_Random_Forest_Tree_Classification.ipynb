{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Membuat session Spark\n",
    "# spark = SparkSession.builder.appName(\"OneHotExample\").getOrCreate()\n",
    "\n",
    "# # Data contoh\n",
    "# data = spark.createDataFrame([\n",
    "#     (\"Indonesia\", 1),\n",
    "#     (\"USA\", 0),\n",
    "#     (\"India\", 1),\n",
    "#     (\"Canada\", 0),\n",
    "#     (\"Brazil\", 1)\n",
    "# ], [\"negara\", \"label\"])\n",
    "\n",
    "# # Langkah 1: StringIndexer untuk kolom 'negara'\n",
    "# indexer = StringIndexer(inputCol=\"negara\", outputCol=\"negara_index\")\n",
    "# indexed_data = indexer.fit(data).transform(data)\n",
    "\n",
    "# # Langkah 2: OneHotEncoder untuk kolom yang sudah diindeks\n",
    "# encoder = OneHotEncoder(inputCol=\"negara_index\", outputCol=\"negara_onehot\")\n",
    "# encoded_data = encoder.fit(indexed_data).transform(indexed_data)\n",
    "\n",
    "# # Langkah 3: Gabungkan fitur (menggunakan VectorAssembler untuk membuat vektor fitur)\n",
    "# assembler = VectorAssembler(inputCols=[\"negara_onehot\"], outputCol=\"features\")\n",
    "# assembled_data = assembler.transform(encoded_data)\n",
    "\n",
    "# # Langkah 4: Melatih model Logistic Regression\n",
    "# lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "# model = lr.fit(assembled_data)\n",
    "\n",
    "# # Menampilkan hasil model\n",
    "# print(\"Model telah dilatih dengan sukses!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1XACuTaxG8sC"
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "# findspark.find()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler, MinMaxScaler, OneHotEncoder, StringIndexer\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ruJySHFrG8sE"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Classification with Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "    \n",
      "    This function will go through the input once to determine the input schema if\n",
      "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path : str or list\n",
      "        string, or list of strings, for input path(s),\n",
      "        or RDD of Strings storing CSV rows.\n",
      "    schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "        an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "        for the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Write a DataFrame into a CSV file and read it back.\n",
      "    \n",
      "    >>> import tempfile\n",
      "    >>> with tempfile.TemporaryDirectory() as d:\n",
      "    ...     # Write a DataFrame into a CSV file\n",
      "    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      "    ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
      "    ...\n",
      "    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      "    ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
      "    +---+----+\n",
      "    |age|name|\n",
      "    +---+----+\n",
      "    |100|NULL|\n",
      "    +---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W_HU1tenG8sE"
   },
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"diabetes.csv\",header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "esmq2Bw6G8sF",
    "outputId": "ab525593-9630-4b1e-d593-8af873424319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|\n",
      "|          5|    116|           74|            0|      0|25.6|                   0.201| 30|      0|\n",
      "|          3|     78|           50|           32|     88|31.0|                   0.248| 26|      1|\n",
      "|         10|    115|            0|            0|      0|35.3|                   0.134| 29|      0|\n",
      "|          2|    197|           70|           45|    543|30.5|                   0.158| 53|      1|\n",
      "|          8|    125|           96|            0|      0| 0.0|                   0.232| 54|      1|\n",
      "|          4|    110|           92|            0|      0|37.6|                   0.191| 30|      0|\n",
      "|         10|    168|           74|            0|      0|38.0|                   0.537| 34|      1|\n",
      "|         10|    139|           80|            0|      0|27.1|                   1.441| 57|      0|\n",
      "|          1|    189|           60|           23|    846|30.1|                   0.398| 59|      1|\n",
      "|          5|    166|           72|           19|    175|25.8|                   0.587| 51|      1|\n",
      "|          7|    100|            0|            0|      0|30.0|                   0.484| 32|      1|\n",
      "|          0|    118|           84|           47|    230|45.8|                   0.551| 31|      1|\n",
      "|          7|    107|           74|            0|      0|29.6|                   0.254| 31|      1|\n",
      "|          1|    103|           30|           38|     83|43.3|                   0.183| 33|      0|\n",
      "|          1|    115|           70|           30|     96|34.6|                   0.529| 32|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dwgYwbn2G8sF",
    "outputId": "1668132f-2497-4e5e-f1ec-9771b6a936b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pregnancies: integer (nullable = true)\n",
      " |-- Glucose: integer (nullable = true)\n",
      " |-- BloodPressure: integer (nullable = true)\n",
      " |-- SkinThickness: integer (nullable = true)\n",
      " |-- Insulin: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- DiabetesPedigreeFunction: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Outcome: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GyV3IpHFG8sG"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "new_data = dataset.select(*(col(c).cast(\"float\").alias(c) for c in dataset.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bYBCPe5_G8sG",
    "outputId": "dd036b43-22de-480b-f2e9-2ee665adf4fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pregnancies: float (nullable = true)\n",
      " |-- Glucose: float (nullable = true)\n",
      " |-- BloodPressure: float (nullable = true)\n",
      " |-- SkinThickness: float (nullable = true)\n",
      " |-- Insulin: float (nullable = true)\n",
      " |-- BMI: float (nullable = true)\n",
      " |-- DiabetesPedigreeFunction: float (nullable = true)\n",
      " |-- Age: float (nullable = true)\n",
      " |-- Outcome: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+----+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction| Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+----+-------+\n",
      "|        6.0|  148.0|         72.0|         35.0|    0.0|33.6|                   0.627|50.0|    1.0|\n",
      "|        1.0|   85.0|         66.0|         29.0|    0.0|26.6|                   0.351|31.0|    0.0|\n",
      "|        8.0|  183.0|         64.0|          0.0|    0.0|23.3|                   0.672|32.0|    1.0|\n",
      "|        1.0|   89.0|         66.0|         23.0|   94.0|28.1|                   0.167|21.0|    0.0|\n",
      "|        0.0|  137.0|         40.0|         35.0|  168.0|43.1|                   2.288|33.0|    1.0|\n",
      "|        5.0|  116.0|         74.0|          0.0|    0.0|25.6|                   0.201|30.0|    0.0|\n",
      "|        3.0|   78.0|         50.0|         32.0|   88.0|31.0|                   0.248|26.0|    1.0|\n",
      "|       10.0|  115.0|          0.0|          0.0|    0.0|35.3|                   0.134|29.0|    0.0|\n",
      "|        2.0|  197.0|         70.0|         45.0|  543.0|30.5|                   0.158|53.0|    1.0|\n",
      "|        8.0|  125.0|         96.0|          0.0|    0.0| 0.0|                   0.232|54.0|    1.0|\n",
      "|        4.0|  110.0|         92.0|          0.0|    0.0|37.6|                   0.191|30.0|    0.0|\n",
      "|       10.0|  168.0|         74.0|          0.0|    0.0|38.0|                   0.537|34.0|    1.0|\n",
      "|       10.0|  139.0|         80.0|          0.0|    0.0|27.1|                   1.441|57.0|    0.0|\n",
      "|        1.0|  189.0|         60.0|         23.0|  846.0|30.1|                   0.398|59.0|    1.0|\n",
      "|        5.0|  166.0|         72.0|         19.0|  175.0|25.8|                   0.587|51.0|    1.0|\n",
      "|        7.0|  100.0|          0.0|          0.0|    0.0|30.0|                   0.484|32.0|    1.0|\n",
      "|        0.0|  118.0|         84.0|         47.0|  230.0|45.8|                   0.551|31.0|    1.0|\n",
      "|        7.0|  107.0|         74.0|          0.0|    0.0|29.6|                   0.254|31.0|    1.0|\n",
      "|        1.0|  103.0|         30.0|         38.0|   83.0|43.3|                   0.183|33.0|    0.0|\n",
      "|        1.0|  115.0|         70.0|         30.0|   96.0|34.6|                   0.529|32.0|    1.0|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Xarth5g5G8sG",
    "outputId": "76dceb9e-ff61-4fd7-ff19-3f86211538b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+---+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin|BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+---+------------------------+---+-------+\n",
      "|          0|      0|            0|            0|      0|  0|                       0|  0|      0|\n",
      "+-----------+-------+-------------+-------------+-------+---+------------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "#checking for null ir nan type values in our columns\n",
    "new_data.select([count(when(col(c).isNull(), c)).alias(c) for c in new_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9vBruo37G8sG",
    "outputId": "4049b633-5035-4e81-8630-d0364eae957f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+-------+\n",
      "|features                                                               |Outcome|\n",
      "+-----------------------------------------------------------------------+-------+\n",
      "|[6.0,148.0,72.0,35.0,0.0,33.599998474121094,0.6269999742507935,50.0]   |1.0    |\n",
      "|[1.0,85.0,66.0,29.0,0.0,26.600000381469727,0.35100001096725464,31.0]   |0.0    |\n",
      "|[8.0,183.0,64.0,0.0,0.0,23.299999237060547,0.671999990940094,32.0]     |1.0    |\n",
      "|[1.0,89.0,66.0,23.0,94.0,28.100000381469727,0.16699999570846558,21.0]  |0.0    |\n",
      "|[0.0,137.0,40.0,35.0,168.0,43.099998474121094,2.2880001068115234,33.0] |1.0    |\n",
      "|[5.0,116.0,74.0,0.0,0.0,25.600000381469727,0.20100000500679016,30.0]   |0.0    |\n",
      "|[3.0,78.0,50.0,32.0,88.0,31.0,0.24799999594688416,26.0]                |1.0    |\n",
      "|[10.0,115.0,0.0,0.0,0.0,35.29999923706055,0.1340000033378601,29.0]     |0.0    |\n",
      "|[2.0,197.0,70.0,45.0,543.0,30.5,0.15800000727176666,53.0]              |1.0    |\n",
      "|[8.0,125.0,96.0,0.0,0.0,0.0,0.23199999332427979,54.0]                  |1.0    |\n",
      "|[4.0,110.0,92.0,0.0,0.0,37.599998474121094,0.19099999964237213,30.0]   |0.0    |\n",
      "|[10.0,168.0,74.0,0.0,0.0,38.0,0.5370000004768372,34.0]                 |1.0    |\n",
      "|[10.0,139.0,80.0,0.0,0.0,27.100000381469727,1.440999984741211,57.0]    |0.0    |\n",
      "|[1.0,189.0,60.0,23.0,846.0,30.100000381469727,0.39800000190734863,59.0]|1.0    |\n",
      "|[5.0,166.0,72.0,19.0,175.0,25.799999237060547,0.5870000123977661,51.0] |1.0    |\n",
      "|[7.0,100.0,0.0,0.0,0.0,30.0,0.48399999737739563,32.0]                  |1.0    |\n",
      "|[0.0,118.0,84.0,47.0,230.0,45.79999923706055,0.5509999990463257,31.0]  |1.0    |\n",
      "|[7.0,107.0,74.0,0.0,0.0,29.600000381469727,0.2540000081062317,31.0]    |1.0    |\n",
      "|[1.0,103.0,30.0,38.0,83.0,43.29999923706055,0.18299999833106995,33.0]  |0.0    |\n",
      "|[1.0,115.0,70.0,30.0,96.0,34.599998474121094,0.5289999842643738,32.0]  |1.0    |\n",
      "+-----------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols=new_data.columns\n",
    "cols.remove(\"Outcome\")\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "data=assembler.transform(new_data)\n",
    "\n",
    "data.select(\"features\",'Outcome').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "P1PioeJ4G8sH"
   },
   "outputs": [],
   "source": [
    "standardscaler=StandardScaler()\n",
    "sc = standardscaler.setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "data_scaled=sc.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Zk11rmmSG8sH",
    "outputId": "9f3a6998-cca2-4421-f39d-0c22846df24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                               |Outcome|Scaled_features                                                                                                                                          |\n",
      "+-----------------------------------------------------------------------+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[6.0,148.0,72.0,35.0,0.0,33.599998474121094,0.6269999742507935,50.0]   |1.0    |[1.7806383732194306,4.628960915766174,3.7198138711154307,2.1940523222807116,0.0,4.261709202425419,1.8923810993699686,4.251616970894646]                  |\n",
      "|[1.0,85.0,66.0,29.0,0.0,26.600000381469727,0.35100001096725464,31.0]   |0.0    |[0.29677306220323846,2.658524850271114,3.4098293818558116,1.8179290670325896,0.0,3.373853320188119,1.0593713140527197,2.6360025219546803]                |\n",
      "|[8.0,183.0,64.0,0.0,0.0,23.299999237060547,0.671999990940094,32.0]     |1.0    |[2.3741844976259077,5.723647618818986,3.306501218769272,0.0,0.0,2.955292430788826,2.028197980632078,2.721034861372573]                                   |\n",
      "|[1.0,89.0,66.0,23.0,94.0,28.100000381469727,0.16699999570846558,21.0]  |0.0    |[0.29677306220323846,2.783631902048578,3.4098293818558116,1.4418058117844677,0.8156606685129459,3.564108203936454,0.5040313372439763,1.785679127775751]  |\n",
      "|[0.0,137.0,40.0,35.0,168.0,43.099998474121094,2.2880001068115234,33.0] |1.0    |[0.0,4.284916523378148,2.0665632617307947,2.1940523222807116,1.4577765139380312,5.466656799498205,6.905531635244907,2.806067200790466]                   |\n",
      "|[5.0,116.0,74.0,0.0,0.0,25.600000381469727,0.20100000500679016,30.0]   |0.0    |[1.4838653110161923,3.628104501546461,3.823142034201971,0.0,0.0,3.247016731022563,0.6066485264255773,2.5509701825367874]                                 |\n",
      "|[3.0,78.0,50.0,32.0,88.0,31.0,0.24799999594688416,26.0]                |1.0    |[0.8903191866097153,2.4395875096605515,2.5832040771634937,2.0059906946566506,0.7635972215865877,3.9319342641322486,0.7485016335678398,2.210840824865216] |\n",
      "|[10.0,115.0,0.0,0.0,0.0,35.29999923706055,0.1340000033378601,29.0]     |0.0    |[2.9677306220323847,3.596827738602095,0.0,0.0,0.0,4.477331500775503,0.4044323509503849,2.4659378431188945]                                               |\n",
      "|[2.0,197.0,70.0,45.0,543.0,30.5,0.15800000727176666,53.0]              |1.0    |[0.5935461244064769,6.161522300040111,3.616485708028891,2.8209244143609147,4.711741946835422,3.8685159695494704,0.4768680059655209,4.5067139891483246]   |\n",
      "|[8.0,125.0,96.0,0.0,0.0,0.0,0.23199999332427979,54.0]                  |1.0    |[2.3741844976259077,3.9095953680457556,4.959751828153908,0.0,0.0,0.0,0.7002111968910825,4.5917463285662174]                                              |\n",
      "|[4.0,110.0,92.0,0.0,0.0,37.599998474121094,0.19099999964237213,30.0]   |0.0    |[1.1870922488129538,3.440443923880265,4.7530955019808285,0.0,0.0,4.7690555590876444,0.5764669922591124,2.5509701825367874]                               |\n",
      "|[10.0,168.0,74.0,0.0,0.0,38.0,0.5370000004768372,34.0]                 |1.0    |[2.9677306220323847,5.254496174653495,3.823142034201971,0.0,0.0,4.8197903882911435,1.6207475167416163,2.891099540208359]                                 |\n",
      "|[10.0,139.0,80.0,0.0,0.0,27.100000381469727,1.440999984741211,57.0]    |0.0    |[2.9677306220323847,4.34747004926688,4.1331265234615895,0.0,0.0,3.4372716147708973,4.349156694264776,4.846843346819896]                                  |\n",
      "|[1.0,189.0,60.0,23.0,846.0,30.100000381469727,0.39800000190734863,59.0]|1.0    |[0.29677306220323846,5.911308196485182,3.0998448925961926,1.4418058117844677,7.340946016616514,3.8177813822675666,1.201224421194982,5.016908025655682]   |\n",
      "|[5.0,166.0,72.0,19.0,175.0,25.799999237060547,0.5870000123977661,51.0] |1.0    |[1.4838653110161923,5.191942648764763,3.7198138711154307,1.1910569749523863,1.5185172020187825,3.272383903702717,1.7716551425999747,4.336649310312539]   |\n",
      "|[7.0,100.0,0.0,0.0,0.0,30.0,0.48399999737739563,32.0]                  |1.0    |[2.077411435422669,3.127676294436604,0.0,0.0,0.0,3.8050976749666923,1.4607854621150949,2.721034861372573]                                                |\n",
      "|[0.0,118.0,84.0,47.0,230.0,45.79999923706055,0.5509999990463257,31.0]  |1.0    |[0.0,3.6906580274351932,4.33978284963467,2.9462988327769555,1.9957654655103998,5.809115687013845,1.6630016375902872,2.6360025219546803]                  |\n",
      "|[7.0,107.0,74.0,0.0,0.0,29.600000381469727,0.2540000081062317,31.0]    |1.0    |[2.077411435422669,3.3466136350471665,3.823142034201971,0.0,0.0,3.7543630876847884,0.7666105810520987,2.6360025219546803]                                |\n",
      "|[1.0,103.0,30.0,38.0,83.0,43.29999923706055,0.18299999833106995,33.0]  |0.0    |[0.29677306220323846,3.2215065832697025,1.5499224462980963,2.3821139499047725,0.7202110158146225,5.4920242140999544,0.5523217739207337,2.806067200790466]|\n",
      "|[1.0,115.0,70.0,30.0,96.0,34.599998474121094,0.5289999842643738,32.0]  |1.0    |[0.29677306220323846,3.596827738602095,3.616485708028891,1.88061627624061,0.833015150821732,4.388545791590976,1.596602253429271,2.721034861372573]       |\n",
      "+-----------------------------------------------------------------------+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_scaled.select(\"features\",'Outcome','Scaled_features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "izWIuqAsG8sH",
    "outputId": "910d6a11-82b2-47e7-dfc1-7925e1cea71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|     Scaled_features|Outcome|\n",
      "+--------------------+-------+\n",
      "|[1.78063837321943...|    1.0|\n",
      "|[0.29677306220323...|    0.0|\n",
      "|[2.37418449762590...|    1.0|\n",
      "|[0.29677306220323...|    0.0|\n",
      "|[0.0,4.2849165233...|    1.0|\n",
      "|[1.48386531101619...|    0.0|\n",
      "|[0.89031918660971...|    1.0|\n",
      "|[2.96773062203238...|    0.0|\n",
      "|[0.59354612440647...|    1.0|\n",
      "|[2.37418449762590...|    1.0|\n",
      "|[1.18709224881295...|    0.0|\n",
      "|[2.96773062203238...|    1.0|\n",
      "|[2.96773062203238...|    0.0|\n",
      "|[0.29677306220323...|    1.0|\n",
      "|[1.48386531101619...|    1.0|\n",
      "|[2.07741143542266...|    1.0|\n",
      "|[0.0,3.6906580274...|    1.0|\n",
      "|[2.07741143542266...|    1.0|\n",
      "|[0.29677306220323...|    0.0|\n",
      "|[0.29677306220323...|    1.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembled_data = data_scaled.select(\"Scaled_features\",\"Outcome\")\n",
    "assembled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xqYhSGc7G8sH"
   },
   "outputs": [],
   "source": [
    "train, test = assembled_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 65.58966074313409\n",
      "1.0 34.41033925686591\n",
      "==========================================\n",
      "0.0 63.08724832214765\n",
      "1.0 36.91275167785235\n"
     ]
    }
   ],
   "source": [
    "dist=dict(train.select(\"Outcome\").rdd.countByValue()).items()\n",
    "for k,v in dist:\n",
    "    print(k[0],v/train.count()*100)\n",
    "print(\"==========================================\")\n",
    "dist=dict(test.select(\"Outcome\").rdd.countByValue()).items()\n",
    "for k,v in dist:\n",
    "    print(k[0],v/test.count()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembled_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Outcome=1.0), Row(Outcome=0.0)]\n",
      "Row(Outcome=1.0)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(assembled_data.select(\"Outcome\").distinct().rdd.collect())\n",
    "print(assembled_data.select(\"Outcome\").distinct().rdd.collect()[0])\n",
    "print(assembled_data.select(\"Outcome\").distinct().rdd.collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 0.7, 0.0: 0.7}\n",
      "Train Data Count:  569\n",
      "Test Data Count:  199\n",
      "Total Data Count:  768\n"
     ]
    }
   ],
   "source": [
    "fractions = assembled_data.select(\"Outcome\").distinct().rdd.map(lambda row: (row[0], 0.7)).collect()\n",
    "fractions = dict(fractions)\n",
    "print(fractions)\n",
    "\n",
    "train = assembled_data.sampleBy(\"Outcome\",fractions, seed=42)\n",
    "test = assembled_data.subtract(train)\n",
    "\n",
    "print(\"Train Data Count: \", train.count())\n",
    "print(\"Test Data Count: \", test.count())\n",
    "print(\"Total Data Count: \", test.count() + train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+\n",
      "|Outcome|count|       percentage|\n",
      "+-------+-----+-----------------+\n",
      "|    1.0|  195|34.27065026362039|\n",
      "|    0.0|  374|65.72934973637962|\n",
      "+-------+-----+-----------------+\n",
      "\n",
      "+-------+-----+-----------------+\n",
      "|Outcome|count|       percentage|\n",
      "+-------+-----+-----------------+\n",
      "|    1.0|   73|36.68341708542713|\n",
      "|    0.0|  126|63.31658291457286|\n",
      "+-------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_distribution = train.groupBy(\"Outcome\").count()\n",
    "train_distribution = train_distribution.withColumn(\"percentage\", col(\"count\")/train.count()*100)\n",
    "train_distribution.show()\n",
    "\n",
    "test_distribution = test.groupBy(\"Outcome\").count()\n",
    "test_distribution = test_distribution.withColumn(\"percentage\", col(\"count\")/test.count()*100)\n",
    "test_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|     Scaled_features|Outcome|\n",
      "+--------------------+-------+\n",
      "|[1.78063837321943...|    1.0|\n",
      "|[0.29677306220323...|    0.0|\n",
      "|[0.29677306220323...|    0.0|\n",
      "|[0.0,4.2849165233...|    1.0|\n",
      "|[1.48386531101619...|    0.0|\n",
      "|[2.96773062203238...|    0.0|\n",
      "|[1.18709224881295...|    0.0|\n",
      "|[2.96773062203238...|    1.0|\n",
      "|[2.96773062203238...|    0.0|\n",
      "|[0.0,3.6906580274...|    1.0|\n",
      "|[2.07741143542266...|    1.0|\n",
      "|[0.29677306220323...|    0.0|\n",
      "|[0.89031918660971...|    0.0|\n",
      "|[2.07741143542266...|    1.0|\n",
      "|[2.96773062203238...|    1.0|\n",
      "|[2.07741143542266...|    1.0|\n",
      "|[0.29677306220323...|    0.0|\n",
      "|[0.89031918660971...|    1.0|\n",
      "|[1.78063837321943...|    0.0|\n",
      "|[3.26450368423562...|    0.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(569, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count(),train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EV6a1ydeG8sH",
    "outputId": "f370b4c8-41e7-4d22-c7f5-f9d13eee52c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|     Scaled_features|Outcome|\n",
      "+--------------------+-------+\n",
      "|[1.78063837321943...|    0.0|\n",
      "|[2.07741143542266...|    1.0|\n",
      "|[0.89031918660971...|    0.0|\n",
      "|[0.29677306220323...|    0.0|\n",
      "|[0.0,5.6298173299...|    1.0|\n",
      "|[2.67095755982914...|    1.0|\n",
      "|[3.56127674643886...|    1.0|\n",
      "|[0.29677306220323...|    0.0|\n",
      "|[2.07741143542266...|    0.0|\n",
      "|[2.96773062203238...|    0.0|\n",
      "|[1.18709224881295...|    0.0|\n",
      "|[1.18709224881295...|    0.0|\n",
      "|[1.48386531101619...|    1.0|\n",
      "|[0.29677306220323...|    0.0|\n",
      "|[1.48386531101619...|    0.0|\n",
      "|[0.89031918660971...|    0.0|\n",
      "|[2.96773062203238...|    0.0|\n",
      "|[0.59354612440647...|    0.0|\n",
      "|[0.0,3.1589530573...|    0.0|\n",
      "|[0.89031918660971...|    1.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(199, None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count(),test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWHHML9nG8sI"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "EG9UKKStG8sJ"
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"Scaled_features\",maxIter=40) # default maxIter=100\n",
    "model=log_reg.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg.getOrDefault(\"maxIter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dh-isySpG8sJ"
   },
   "outputs": [],
   "source": [
    "prediction_test=model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qFRvuTVPG8sJ",
    "outputId": "5421a881-67bf-4bfd-8189-2246be051cae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|     Scaled_features|Outcome|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.78063837321943...|    0.0|[0.67900848767766...|[0.66351736621234...|       0.0|\n",
      "|[2.07741143542266...|    1.0|[0.91160181629810...|[0.71332783204551...|       0.0|\n",
      "|[0.89031918660971...|    0.0|[1.41249242044969...|[0.80415876615470...|       0.0|\n",
      "|[0.29677306220323...|    0.0|[2.77515687416964...|[0.94131849102158...|       0.0|\n",
      "|[0.0,5.6298173299...|    1.0|[-2.7963699132256...|[0.05752065446034...|       1.0|\n",
      "|[2.67095755982914...|    1.0|[-0.4758345061456...|[0.38323622720395...|       1.0|\n",
      "|[3.56127674643886...|    1.0|[1.25497516102834...|[0.77815989707538...|       0.0|\n",
      "|[0.29677306220323...|    0.0|[-0.3847829506785...|[0.40497382743567...|       1.0|\n",
      "|[2.07741143542266...|    0.0|[1.03363191179512...|[0.73761941157160...|       0.0|\n",
      "|[2.96773062203238...|    0.0|[0.70331813536116...|[0.66892303449076...|       0.0|\n",
      "|[1.18709224881295...|    0.0|[0.61382820954549...|[0.64881357307351...|       0.0|\n",
      "|[1.18709224881295...|    0.0|[1.23962343155976...|[0.77549846027990...|       0.0|\n",
      "|[1.48386531101619...|    1.0|[1.00014607572146...|[0.73108729789061...|       0.0|\n",
      "|[0.29677306220323...|    0.0|[2.69651281962039...|[0.93682055954390...|       0.0|\n",
      "|[1.48386531101619...|    0.0|[2.36966897585430...|[0.91448497729909...|       0.0|\n",
      "|[0.89031918660971...|    0.0|[3.10577271997550...|[0.95713023704988...|       0.0|\n",
      "|[2.96773062203238...|    0.0|[0.25750170630553...|[0.56402205700884...|       0.0|\n",
      "|[0.59354612440647...|    0.0|[1.38224172046584...|[0.79935078907832...|       0.0|\n",
      "|[0.0,3.1589530573...|    0.0|[2.92221895832962...|[0.94893393326093...|       0.0|\n",
      "|[0.89031918660971...|    1.0|[-1.7250436262684...|[0.15122265280201...|       1.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "xqXZqgIiG8sJ",
    "outputId": "a0d2c647-dffe-4091-9c7d-20ee6a90ff78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Outcome|prediction|\n",
      "+-------+----------+\n",
      "|    0.0|       0.0|\n",
      "|    1.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "|    1.0|       1.0|\n",
      "|    1.0|       1.0|\n",
      "|    1.0|       0.0|\n",
      "|    0.0|       1.0|\n",
      "|    0.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_test.select(\"Outcome\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     Scaled_features|\n",
      "+--------------------+\n",
      "|[1.78063837321943...|\n",
      "|[2.07741143542266...|\n",
      "|[0.89031918660971...|\n",
      "|[0.29677306220323...|\n",
      "|[0.0,5.6298173299...|\n",
      "|[2.67095755982914...|\n",
      "|[3.56127674643886...|\n",
      "|[0.29677306220323...|\n",
      "|[2.07741143542266...|\n",
      "|[2.96773062203238...|\n",
      "|[1.18709224881295...|\n",
      "|[1.18709224881295...|\n",
      "|[1.48386531101619...|\n",
      "|[0.29677306220323...|\n",
      "|[1.48386531101619...|\n",
      "|[0.89031918660971...|\n",
      "|[2.96773062203238...|\n",
      "|[0.59354612440647...|\n",
      "|[0.0,3.1589530573...|\n",
      "|[0.89031918660971...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_pred = test.select(\"Scaled_features\")\n",
    "q_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|     Scaled_features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[1.78063837321943...|[0.67900848767766...|[0.66351736621234...|       0.0|\n",
      "|[2.07741143542266...|[0.91160181629810...|[0.71332783204551...|       0.0|\n",
      "|[0.89031918660971...|[1.41249242044969...|[0.80415876615470...|       0.0|\n",
      "|[0.29677306220323...|[2.77515687416964...|[0.94131849102158...|       0.0|\n",
      "|[0.0,5.6298173299...|[-2.7963699132256...|[0.05752065446034...|       1.0|\n",
      "|[2.67095755982914...|[-0.4758345061456...|[0.38323622720395...|       1.0|\n",
      "|[3.56127674643886...|[1.25497516102834...|[0.77815989707538...|       0.0|\n",
      "|[0.29677306220323...|[-0.3847829506785...|[0.40497382743567...|       1.0|\n",
      "|[2.07741143542266...|[1.03363191179512...|[0.73761941157160...|       0.0|\n",
      "|[2.96773062203238...|[0.70331813536116...|[0.66892303449076...|       0.0|\n",
      "|[1.18709224881295...|[0.61382820954549...|[0.64881357307351...|       0.0|\n",
      "|[1.18709224881295...|[1.23962343155976...|[0.77549846027990...|       0.0|\n",
      "|[1.48386531101619...|[1.00014607572146...|[0.73108729789061...|       0.0|\n",
      "|[0.29677306220323...|[2.69651281962039...|[0.93682055954390...|       0.0|\n",
      "|[1.48386531101619...|[2.36966897585430...|[0.91448497729909...|       0.0|\n",
      "|[0.89031918660971...|[3.10577271997550...|[0.95713023704988...|       0.0|\n",
      "|[2.96773062203238...|[0.25750170630553...|[0.56402205700884...|       0.0|\n",
      "|[0.59354612440647...|[1.38224172046584...|[0.79935078907832...|       0.0|\n",
      "|[0.0,3.1589530573...|[2.92221895832962...|[0.94893393326093...|       0.0|\n",
      "|[0.89031918660971...|[-1.7250436262684...|[0.15122265280201...|       1.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.transform(q_pred)\n",
    "pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "QBnakj1YG8sJ"
   },
   "outputs": [],
   "source": [
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = prediction_test.select(\"Outcome\",\"prediction\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "0l_bg-FpG8sJ",
    "outputId": "f6859509-ddd6-4b9f-b4d1-34d1cbd83945"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=1.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=1.0),\n",
       " Row(Outcome=0.0, prediction=0.0),\n",
       " Row(Outcome=0.0, prediction=0.0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionAndLabels.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MulticlassClassificationEvaluator in module pyspark.ml.evaluation:\n",
      "\n",
      "class MulticlassClassificationEvaluator(JavaEvaluator, pyspark.ml.param.shared.HasLabelCol, pyspark.ml.param.shared.HasPredictionCol, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.param.shared.HasProbabilityCol, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
      " |  MulticlassClassificationEvaluator(*, predictionCol: str = 'prediction', labelCol: str = 'label', metricName: 'MulticlassClassificationEvaluatorMetricType' = 'f1', weightCol: Optional[str] = None, metricLabel: float = 0.0, beta: float = 1.0, probabilityCol: str = 'probability', eps: float = 1e-15)\n",
      " |  \n",
      " |  Evaluator for Multiclass Classification, which expects input\n",
      " |  columns: prediction, label, weight (optional) and probabilityCol (only for logLoss).\n",
      " |  \n",
      " |  .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> scoreAndLabels = [(0.0, 0.0), (0.0, 1.0), (0.0, 0.0),\n",
      " |  ...     (1.0, 0.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (2.0, 2.0), (2.0, 0.0)]\n",
      " |  >>> dataset = spark.createDataFrame(scoreAndLabels, [\"prediction\", \"label\"])\n",
      " |  >>> evaluator = MulticlassClassificationEvaluator()\n",
      " |  >>> evaluator.setPredictionCol(\"prediction\")\n",
      " |  MulticlassClassificationEvaluator...\n",
      " |  >>> evaluator.evaluate(dataset)\n",
      " |  0.66...\n",
      " |  >>> evaluator.evaluate(dataset, {evaluator.metricName: \"accuracy\"})\n",
      " |  0.66...\n",
      " |  >>> evaluator.evaluate(dataset, {evaluator.metricName: \"truePositiveRateByLabel\",\n",
      " |  ...     evaluator.metricLabel: 1.0})\n",
      " |  0.75...\n",
      " |  >>> evaluator.setMetricName(\"hammingLoss\")\n",
      " |  MulticlassClassificationEvaluator...\n",
      " |  >>> evaluator.evaluate(dataset)\n",
      " |  0.33...\n",
      " |  >>> mce_path = temp_path + \"/mce\"\n",
      " |  >>> evaluator.save(mce_path)\n",
      " |  >>> evaluator2 = MulticlassClassificationEvaluator.load(mce_path)\n",
      " |  >>> str(evaluator2.getPredictionCol())\n",
      " |  'prediction'\n",
      " |  >>> scoreAndLabelsAndWeight = [(0.0, 0.0, 1.0), (0.0, 1.0, 1.0), (0.0, 0.0, 1.0),\n",
      " |  ...     (1.0, 0.0, 1.0), (1.0, 1.0, 1.0), (1.0, 1.0, 1.0), (1.0, 1.0, 1.0),\n",
      " |  ...     (2.0, 2.0, 1.0), (2.0, 0.0, 1.0)]\n",
      " |  >>> dataset = spark.createDataFrame(scoreAndLabelsAndWeight, [\"prediction\", \"label\", \"weight\"])\n",
      " |  >>> evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",\n",
      " |  ...     weightCol=\"weight\")\n",
      " |  >>> evaluator.evaluate(dataset)\n",
      " |  0.66...\n",
      " |  >>> evaluator.evaluate(dataset, {evaluator.metricName: \"accuracy\"})\n",
      " |  0.66...\n",
      " |  >>> predictionAndLabelsWithProbabilities = [\n",
      " |  ...      (1.0, 1.0, 1.0, [0.1, 0.8, 0.1]), (0.0, 2.0, 1.0, [0.9, 0.05, 0.05]),\n",
      " |  ...      (0.0, 0.0, 1.0, [0.8, 0.2, 0.0]), (1.0, 1.0, 1.0, [0.3, 0.65, 0.05])]\n",
      " |  >>> dataset = spark.createDataFrame(predictionAndLabelsWithProbabilities, [\"prediction\",\n",
      " |  ...     \"label\", \"weight\", \"probability\"])\n",
      " |  >>> evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",\n",
      " |  ...     probabilityCol=\"probability\")\n",
      " |  >>> evaluator.setMetricName(\"logLoss\")\n",
      " |  MulticlassClassificationEvaluator...\n",
      " |  >>> evaluator.evaluate(dataset)\n",
      " |  0.9682...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MulticlassClassificationEvaluator\n",
      " |      JavaEvaluator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      Evaluator\n",
      " |      pyspark.ml.param.shared.HasLabelCol\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      pyspark.ml.param.shared.HasWeightCol\n",
      " |      pyspark.ml.param.shared.HasProbabilityCol\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      typing.Generic\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, predictionCol: str = 'prediction', labelCol: str = 'label', metricName: 'MulticlassClassificationEvaluatorMetricType' = 'f1', weightCol: Optional[str] = None, metricLabel: float = 0.0, beta: float = 1.0, probabilityCol: str = 'probability', eps: float = 1e-15)\n",
      " |      __init__(self, \\*, predictionCol=\"prediction\", labelCol=\"label\",                  metricName=\"f1\", weightCol=None, metricLabel=0.0, beta=1.0,                  probabilityCol=\"probability\", eps=1e-15)\n",
      " |  \n",
      " |  getBeta(self) -> float\n",
      " |      Gets the value of beta or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  getEps(self) -> float\n",
      " |      Gets the value of eps or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  getMetricLabel(self) -> float\n",
      " |      Gets the value of metricLabel or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  getMetricName(self) -> 'MulticlassClassificationEvaluatorMetricType'\n",
      " |      Gets the value of metricName or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  setBeta(self, value: float) -> 'MulticlassClassificationEvaluator'\n",
      " |      Sets the value of :py:attr:`beta`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setEps(self, value: float) -> 'MulticlassClassificationEvaluator'\n",
      " |      Sets the value of :py:attr:`eps`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setLabelCol(self, value: str) -> 'MulticlassClassificationEvaluator'\n",
      " |      Sets the value of :py:attr:`labelCol`.\n",
      " |  \n",
      " |  setMetricLabel(self, value: float) -> 'MulticlassClassificationEvaluator'\n",
      " |      Sets the value of :py:attr:`metricLabel`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setMetricName(self, value: 'MulticlassClassificationEvaluatorMetricType') -> 'MulticlassClassificationEvaluator'\n",
      " |      Sets the value of :py:attr:`metricName`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  setParams(self, *, predictionCol: str = 'prediction', labelCol: str = 'label', metricName: 'MulticlassClassificationEvaluatorMetricType' = 'f1', weightCol: Optional[str] = None, metricLabel: float = 0.0, beta: float = 1.0, probabilityCol: str = 'probability', eps: float = 1e-15) -> 'MulticlassClassificationEvaluator'\n",
      " |      setParams(self, \\*, predictionCol=\"prediction\", labelCol=\"label\",                   metricName=\"f1\", weightCol=None, metricLabel=0.0, beta=1.0,                   probabilityCol=\"probability\", eps=1e-15)\n",
      " |      Sets params for multiclass classification evaluator.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  setPredictionCol(self, value: str) -> 'MulticlassClassificationEvaluator'\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |  \n",
      " |  setProbabilityCol(self, value: str) -> 'MulticlassClassificationEvaluator'\n",
      " |      Sets the value of :py:attr:`probabilityCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setWeightCol(self, value: str) -> 'MulticlassClassificationEvaluator'\n",
      " |      Sets the value of :py:attr:`weightCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any], 'bet...\n",
      " |  \n",
      " |  __orig_bases__ = (<class 'pyspark.ml.evaluation.JavaEvaluator'>, <clas...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  beta = Param(parent='undefined', name='beta', doc='The ...reByLabel. M...\n",
      " |  \n",
      " |  eps = Param(parent='undefined', name='eps', doc='log-l... in range (0,...\n",
      " |  \n",
      " |  metricLabel = Param(parent='undefined', name='metricLabel', do...eByLa...\n",
      " |  \n",
      " |  metricName = Param(parent='undefined', name='metricName', doc...llByLa...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from JavaEvaluator:\n",
      " |  \n",
      " |  isLargerBetter(self) -> bool\n",
      " |      Indicates whether the metric returned by :py:meth:`evaluate` should be maximized\n",
      " |      (True, default) or minimized (False).\n",
      " |      A given evaluator may support multiple metrics which may be maximized or minimized.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  clear(self, param: pyspark.ml.param.Param) -> None\n",
      " |      Clears a param from the param map if it has been explicitly set.\n",
      " |  \n",
      " |  copy(self: 'JP', extra: Optional[ForwardRef('ParamMap')] = None) -> 'JP'\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          Extra parameters to copy to the new instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`JavaParams`\n",
      " |          Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Evaluator:\n",
      " |  \n",
      " |  evaluate(self, dataset: pyspark.sql.dataframe.DataFrame, params: Optional[ForwardRef('ParamMap')] = None) -> float\n",
      " |      Evaluates the output with optional parameters.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          a dataset that contains labels/observations and predictions\n",
      " |      params : dict, optional\n",
      " |          an optional param map that overrides embedded params\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          metric\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  getLabelCol(self) -> str\n",
      " |      Gets the value of labelCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self) -> str\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  getWeightCol(self) -> str\n",
      " |      Gets the value of weightCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n",
      " |  \n",
      " |  getProbabilityCol(self) -> str\n",
      " |      Gets the value of probabilityCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n",
      " |  \n",
      " |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self) -> str\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra: Optional[ForwardRef('ParamMap')] = None) -> 'ParamMap'\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          extra param values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName: str) -> bool\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path: str) -> ~RL from abc.ABCMeta\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self) -> pyspark.ml.util.JavaMLWriter\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path: str) -> None\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MulticlassClassificationEvaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "eq81SXpuG8sJ",
    "outputId": "501ea3d2-407b-41e5-eebc-d3f2260dbb77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7537688442211056\n",
      "Precision: 0.7490750455574577\n",
      "Recall: 0.7537688442211055\n",
      "F1 Score: 0.7453248490804213\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_lr = evaluator.evaluate(prediction_test)\n",
    "print(f\"Accuracy: {accuracy_lr}\")\n",
    "\n",
    "# Evaluasi precision, recall, dan F1-score\n",
    "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(prediction_test)\n",
    "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(prediction_test)\n",
    "f1_score = evaluator.setMetricName(\"f1\").evaluate(prediction_test)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(BinaryClassificationMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Anaconda3\\envs\\PySparkEnv\\lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.7417582417582418\n",
      "Area under PR = 0.509868324007513\n"
     ]
    }
   ],
   "source": [
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVQ4vZZUG8sJ"
   },
   "source": [
    "## NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(NaiveBayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "XOYPbfE7G8sJ"
   },
   "outputs": [],
   "source": [
    "naive_bayes = NaiveBayes(featuresCol='Scaled_features',labelCol='Outcome',smoothing=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "SRAbTR68G8sK"
   },
   "outputs": [],
   "source": [
    "model = naive_bayes.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "P3ZI-J9cG8sK"
   },
   "outputs": [],
   "source": [
    "# select example rows to display.\n",
    "prediction_test = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "xQsnVIISG8sK",
    "outputId": "e01222ac-0984-41fd-8540-bc3bd534c8b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|     Scaled_features|Outcome|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.78063837321943...|    0.0|[-46.361374930926...|[0.63156663200021...|       0.0|\n",
      "|[2.07741143542266...|    1.0|[-52.930266416654...|[0.54933129458851...|       0.0|\n",
      "|[0.89031918660971...|    0.0|[-31.153240407360...|[0.64732025283577...|       0.0|\n",
      "|[0.29677306220323...|    0.0|[-27.915121723352...|[0.70519574803775...|       0.0|\n",
      "|[0.0,5.6298173299...|    1.0|[-48.412500439396...|[0.73541714453796...|       0.0|\n",
      "|[2.67095755982914...|    1.0|[-39.567752893829...|[0.51964275877738...|       0.0|\n",
      "|[3.56127674643886...|    1.0|[-48.999990469351...|[0.44190439767670...|       1.0|\n",
      "|[0.29677306220323...|    0.0|[-42.796681776404...|[0.57791194290637...|       0.0|\n",
      "|[2.07741143542266...|    0.0|[-41.839411641841...|[0.61396175206550...|       0.0|\n",
      "|[2.96773062203238...|    0.0|[-51.590829758963...|[0.58615120894702...|       0.0|\n",
      "|[1.18709224881295...|    0.0|[-29.103544136272...|[0.64117125295485...|       0.0|\n",
      "|[1.18709224881295...|    0.0|[-29.516447845276...|[0.64021563000729...|       0.0|\n",
      "|[1.48386531101619...|    1.0|[-40.908834082087...|[0.6653845943174,...|       0.0|\n",
      "|[0.29677306220323...|    0.0|[-29.936645462417...|[0.70399545179951...|       0.0|\n",
      "|[1.48386531101619...|    0.0|[-34.008439567632...|[0.69315506199071...|       0.0|\n",
      "|[0.89031918660971...|    0.0|[-26.140597170621...|[0.66324326803497...|       0.0|\n",
      "|[2.96773062203238...|    0.0|[-43.529946687151...|[0.58220971266147...|       0.0|\n",
      "|[0.59354612440647...|    0.0|[-37.185480005829...|[0.61756430450194...|       0.0|\n",
      "|[0.0,3.1589530573...|    0.0|[-25.743543000304...|[0.74833035559787...|       0.0|\n",
      "|[0.89031918660971...|    1.0|[-36.447281306460...|[0.67400335053320...|       0.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "JN-HL4gEG8sK",
    "outputId": "2824d008-3b7f-4129-ec58-9ebc629b537a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Outcome|prediction|\n",
      "+-------+----------+\n",
      "|    0.0|       0.0|\n",
      "|    1.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "|    1.0|       0.0|\n",
      "|    1.0|       0.0|\n",
      "|    1.0|       1.0|\n",
      "|    0.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_test.select(\"Outcome\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "FoXN5yFGG8sK"
   },
   "outputs": [],
   "source": [
    "predictionAndLabels = prediction_test.select(\"Outcome\",\"prediction\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "H6pQKTUZG8sK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6381909547738693\n",
      "Precision: 0.6152413854989232\n",
      "Recall: 0.6381909547738693\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_nb = evaluator.evaluate(prediction_test)\n",
    "print(f\"Accuracy: {accuracy_nb}\")\n",
    "\n",
    "# Evaluasi precision, recall, dan F1-score\n",
    "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(prediction_test)\n",
    "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(prediction_test)\n",
    "f1_score = evaluator.setMetricName(\"f1\").evaluate(prediction_test)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(BinaryClassificationMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ZJlYJOEfG8sK",
    "outputId": "2349c85c-3e3e-40f6-a53b-804c5234264a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Anaconda3\\envs\\PySparkEnv\\lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.6060267857142857\n",
      "Area under PR = 0.050590526015596576\n"
     ]
    }
   ],
   "source": [
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_java_model',\n",
       " '_sc',\n",
       " 'areaUnderPR',\n",
       " 'areaUnderROC',\n",
       " 'call',\n",
       " 'unpersist']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJv95RogG8sK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZpLUPFMG8sK"
   },
   "source": [
    "## GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(GBTClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "oUyPr0OkG8sK"
   },
   "outputs": [],
   "source": [
    "gradient_boost_class = GBTClassifier(labelCol=\"Outcome\", featuresCol=\"Scaled_features\", maxIter=15, maxDepth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "gY5Kgr4-G8sK"
   },
   "outputs": [],
   "source": [
    "model = gradient_boost_class.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "o_NyzOhqG8sO"
   },
   "outputs": [],
   "source": [
    "prediction_test = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "pCgJvJLKG8sO",
    "outputId": "996a50d3-84e4-4fcd-ecd8-3b9bdc87ad4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|     Scaled_features|Outcome|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.78063837321943...|    0.0|[0.15900642016410...|[0.57883988991847...|       0.0|\n",
      "|[2.07741143542266...|    1.0|[0.11851246867129...|[0.55898036211403...|       0.0|\n",
      "|[0.89031918660971...|    0.0|[1.10388238936473...|[0.90094462675756...|       0.0|\n",
      "|[0.29677306220323...|    0.0|[1.44477519839414...|[0.94732745170723...|       0.0|\n",
      "|[0.0,5.6298173299...|    1.0|[-1.0177412580408...|[0.11552753159056...|       1.0|\n",
      "|[2.67095755982914...|    1.0|[-0.5770369130842...|[0.23974576892522...|       1.0|\n",
      "|[3.56127674643886...|    1.0|[-0.2085155765135...|[0.39722738694457...|       1.0|\n",
      "|[0.29677306220323...|    0.0|[-0.8119539910943...|[0.16466661660882...|       1.0|\n",
      "|[2.07741143542266...|    0.0|[0.61485937313149...|[0.77376934449642...|       0.0|\n",
      "|[2.96773062203238...|    0.0|[0.36913037348860...|[0.67661541246938...|       0.0|\n",
      "|[1.18709224881295...|    0.0|[0.34156402612487...|[0.66443648829091...|       0.0|\n",
      "|[1.18709224881295...|    0.0|[0.73321701146107...|[0.81251476990803...|       0.0|\n",
      "|[1.48386531101619...|    1.0|[1.10284916626779...|[0.90076005720451...|       0.0|\n",
      "|[0.29677306220323...|    0.0|[1.33978019257587...|[0.93580972103939...|       0.0|\n",
      "|[1.48386531101619...|    0.0|[0.67564965764685...|[0.79434196916870...|       0.0|\n",
      "|[0.89031918660971...|    0.0|[1.44477519839414...|[0.94732745170723...|       0.0|\n",
      "|[2.96773062203238...|    0.0|[-0.2556845395165...|[0.37487263632697...|       1.0|\n",
      "|[0.59354612440647...|    0.0|[0.42791473804628...|[0.70178857655958...|       0.0|\n",
      "|[0.0,3.1589530573...|    0.0|[1.34632515403745...|[0.93659155989874...|       0.0|\n",
      "|[0.89031918660971...|    1.0|[-0.5441535633130...|[0.25193719150742...|       1.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "aXaVXwkXG8sO",
    "outputId": "40657a06-7f04-4796-93a4-6d1dae0770b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Outcome|prediction|\n",
      "+-------+----------+\n",
      "|    0.0|       0.0|\n",
      "|    1.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "|    1.0|       1.0|\n",
      "|    1.0|       1.0|\n",
      "|    1.0|       1.0|\n",
      "|    0.0|       1.0|\n",
      "|    0.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_test.select(\"Outcome\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "MJrr7fcTG8sP"
   },
   "outputs": [],
   "source": [
    "predictionAndLabels = prediction_test.select(\"Outcome\",\"prediction\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7587939698492462\n",
      "Precision: 0.7587939698492463\n",
      "Recall: 0.7587939698492463\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_gbt = evaluator.evaluate(prediction_test)\n",
    "print(f\"Accuracy: {accuracy_gbt}\")\n",
    "\n",
    "# Evaluasi precision, recall, dan F1-score\n",
    "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(prediction_test)\n",
    "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(prediction_test)\n",
    "f1_score = evaluator.setMetricName(\"f1\").evaluate(prediction_test)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.7403783431180692\n",
      "Area under PR = 0.621194733283607\n"
     ]
    }
   ],
   "source": [
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4JmlhGNG8sP"
   },
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(labelCol=\"Outcome\", featuresCol=\"Scaled_features\", numTrees=50, maxDepth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "91V9XqGPG8sP"
   },
   "outputs": [],
   "source": [
    "model = random_forest_classifier.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "BG6IPQr0G8sP"
   },
   "outputs": [],
   "source": [
    "prediction_test = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "QtV9kR6TG8sP",
    "outputId": "bcf44d98-340a-4af1-a388-bd5042a26581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|     Scaled_features|Outcome|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.78063837321943...|    0.0|[28.2431135531135...|[0.56486227106227...|       0.0|\n",
      "|[2.07741143542266...|    1.0|[19.2615384615384...|[0.38523076923076...|       1.0|\n",
      "|[0.89031918660971...|    0.0|[41.7308603613215...|[0.83461720722643...|       0.0|\n",
      "|[0.29677306220323...|    0.0|          [50.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|[0.0,5.6298173299...|    1.0|[18.9333333333333...|[0.37866666666666...|       1.0|\n",
      "|[2.67095755982914...|    1.0|[19.7982298301694...|[0.39596459660338...|       1.0|\n",
      "|[3.56127674643886...|    1.0|[17.9333333333333...|[0.35866666666666...|       1.0|\n",
      "|[0.29677306220323...|    0.0|         [19.0,31.0]|         [0.38,0.62]|       1.0|\n",
      "|[2.07741143542266...|    0.0|[36.2509259259259...|[0.72501851851851...|       0.0|\n",
      "|[2.96773062203238...|    0.0|[31.7333333333333...|[0.63466666666666...|       0.0|\n",
      "|[1.18709224881295...|    0.0|[23.3849673002630...|[0.46769934600526...|       1.0|\n",
      "|[1.18709224881295...|    0.0|[36.7576842884046...|[0.73515368576809...|       0.0|\n",
      "|[1.48386531101619...|    1.0|[37.9630473653481...|[0.75926094730696...|       0.0|\n",
      "|[0.29677306220323...|    0.0|          [47.0,3.0]|         [0.94,0.06]|       0.0|\n",
      "|[1.48386531101619...|    0.0|[44.1648351648351...|[0.88329670329670...|       0.0|\n",
      "|[0.89031918660971...|    0.0|          [46.5,3.5]|         [0.93,0.07]|       0.0|\n",
      "|[2.96773062203238...|    0.0|[10.8673991965481...|[0.21734798393096...|       1.0|\n",
      "|[0.59354612440647...|    0.0|[33.1101631004422...|[0.66220326200884...|       0.0|\n",
      "|[0.0,3.1589530573...|    0.0|[49.2333333333333...|[0.98466666666666...|       0.0|\n",
      "|[0.89031918660971...|    1.0|[24.7071428571428...|[0.49414285714285...|       1.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "kcAbZRkjG8sP",
    "outputId": "ae2bf068-5427-4435-ccda-731936ffe650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Outcome|prediction|\n",
      "+-------+----------+\n",
      "|    0.0|       0.0|\n",
      "|    1.0|       1.0|\n",
      "|    0.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "|    1.0|       1.0|\n",
      "|    1.0|       1.0|\n",
      "|    1.0|       1.0|\n",
      "|    0.0|       1.0|\n",
      "|    0.0|       0.0|\n",
      "|    0.0|       0.0|\n",
      "+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_test.select(\"Outcome\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "LlD4685DG8sP"
   },
   "outputs": [],
   "source": [
    "predictionAndLabels = prediction_test.select(\"Outcome\",\"prediction\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7537688442211056\n",
      "Precision: 0.7545016976775771\n",
      "Recall: 0.7537688442211055\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_rf = evaluator.evaluate(prediction_test)\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "\n",
    "# Evaluasi precision, recall, dan F1-score\n",
    "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(prediction_test)\n",
    "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(prediction_test)\n",
    "f1_score = evaluator.setMetricName(\"f1\").evaluate(prediction_test)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.7350810810810812\n",
      "Area under PR = 0.620663015187005\n"
     ]
    }
   ],
   "source": [
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "fkmkm5YfG8sQ",
    "outputId": "06db56a1-8da2-412c-ce2a-17f4343dbbbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GBT : 0.7587939698492462\n",
      "Accuracy of LR : 0.7537688442211056\n",
      "Accuracy of RF : 0.7537688442211056\n",
      "Accuracy of NB : 0.6381909547738693\n"
     ]
    }
   ],
   "source": [
    "acc_dict = {\"GBT\":accuracy_gbt,\"LR\":accuracy_lr,\"NB\":accuracy_nb,\"RF\":accuracy_rf}\n",
    "acc_dict = sorted(acc_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for k,v in acc_dict:\n",
    "    print(f\"Accuracy of {k} : {v}\")\n",
    "\n",
    "# print(\"Accuracy of GBT : \",accuracy_gbt)\n",
    "# print(\"Accuracy of LR : \",accuracy_lr)\n",
    "# print(\"Accuracy of NB : \",accuracy_nb)\n",
    "# print(\"Accuracy of RF : \",accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iK3sFF9EG8sQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLOpetAIG8sQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySparkEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
